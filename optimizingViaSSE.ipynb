{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing SSE\n",
    "\n",
    "### optimal within a model\n",
    "We can take the idea that a smaller SSE suggests a better model fit further.\n",
    "Instead of using SSE to compare different models, we can use the SSE to evaluate different parameter values inside the same model.\n",
    "\n",
    "Consider the same dataset as above and suppose we're fitting a simple linear regression.\n",
    "Then our SSE becomes\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{SSE}(y,\\hat{y}_{i}) &= \\sum_{i=1}^{N} [y_{i} - \\hat{y}_{i}]^2\\\\\n",
    "    \\text{SSE}(y,x,\\beta_{0},\\beta_{1}) &= \\sum_{i=1}^{N} [y_{i} - (\\beta_{0} + \\beta_{1}x_{i})]^2\\\\\n",
    "\\end{align}\n",
    "\n",
    "where $y$ and $x$ are vectors of data.\n",
    "Step two in above equation replaced the predicted value $\\hat{y}$ with the linear model used to make this prediction $\\beta_{0}+\\beta_{1}x$.\n",
    "\n",
    "Now our SSE is a function of the data, that cannot be changed, and the parameters of our model $\\beta_{0}$ and $\\beta_{1}$. \n",
    "Changing $\\beta_{0}$ or $\\beta_{1}$ will change the value of the SSE.\n",
    "One way to find a best fit model is to find those parameters value that make the SSE as small as possible. \n",
    "\n",
    "\n",
    "### derivative\n",
    "\n",
    "SSE is a function of $\\beta_{0}$ and $\\beta_{1}$, and can be optimized by taking the derivative with respect to both parameters and finding the point where the derivative of these two equations equals zero simultaneously.\n",
    "\n",
    "We take the derivative with respect to $\\beta_{0}$\n",
    "\n",
    "\\begin{align}\n",
    "   \\frac{ d \\text{SSE}(\\beta_{0},\\beta_{1})}{d \\beta_{0}} &= \\sum_{i=1}^{N} [y_{i} - (\\beta_{0} + \\beta_{1}x_{i})]^2\\\\\n",
    "   \\frac{ d \\text{SSE}(\\beta_{0},\\beta_{1})}{d \\beta_{0}} &= \\sum_{i=1}^{N} \\frac{d}{d \\beta_{0}} [y_{i} - (\\beta_{0} + \\beta_{1}x_{i})]^2\\\\\n",
    "  \\frac{ d \\text{SSE}(\\beta_{0},\\beta_{1})}{d \\beta_{0}}  &= \\sum_{i=1}^{N} -2[y_{i} - (\\beta_{0} + \\beta_{1}x_{i})]\\\\\n",
    "\\end{align}\n",
    "\n",
    "The above derivative can be set to zero and solved for $\\beta_{0}$, our variable.\n",
    "\n",
    "\\begin{align}\n",
    "    \\sum_{i=1}^{N} -2[y_{i} - (\\beta_{0} + \\beta_{1}x_{i})] &= 0\\\\\n",
    "    \\sum_{i=1}^{N} y_{i} - N\\beta_{0} - \\beta_{1}\\sum_{i=1}^{N} x_{i} &=0\\\\\n",
    "    N\\beta_{0} &= \\sum_{i=1}^{N} y_{i} - \\beta_{1}\\sum_{i=1}^{N} x_{i}\\\\\n",
    "     \\beta_{0} &= \\bar{y} - \\beta_{1}\\bar{x}\\\\\n",
    "\\end{align}\n",
    "\n",
    "The value for $\\beta_{0}$ that optimizes the SSE is the average of our $y$ values minus the optimal $\\beta_{1}$ times the average of our $x$ values. \n",
    "\n",
    "We must also take the derivative with respect to $\\beta_{1}$.\n",
    "\n",
    "\\begin{align}\n",
    "   \\frac{ d \\text{SSE}(\\beta_{0},\\beta_{1})}{d \\beta_{1}} &= \\sum_{i=1}^{N} [y_{i} - (\\beta_{0} + \\beta_{1}x_{i})]^2\\\\\n",
    "   \\frac{ d \\text{SSE}(\\beta_{0},\\beta_{1})}{d \\beta_{1}} &= \\sum_{i=1}^{N} -2x_{i}[y_{i} - (\\beta_{0} + \\beta_{1}x_{i})]\\\\\n",
    "\\end{align}\n",
    "\n",
    "The above equation can also be set to zero and solved for $\\beta_{1}$.\n",
    "\n",
    "\\begin{align}\n",
    "    \\sum_{i=1}^{N} -2x_{i}[y_{i} - (\\beta_{0} + \\beta_{1}x_{i})] &=0 \\\\\n",
    "    \\sum_{i=1}^{N} x_{i}y_{i} - x_{i}\\beta_{0} - \\beta_{1}x^{2}_{i} &=0 \\\\\n",
    "\\end{align}    \n",
    "   \n",
    "At this point we can substitute the optimal value for $\\beta_{0}$ we derived.\n",
    "\n",
    "\\begin{align}\n",
    "    \\sum_{i=1}^{N} x_{i}y_{i} - x_{i}(\\bar{y} - \\beta_{1}\\bar{x}) - \\beta_{1}x^{2}_{i} &=0 \\\\    \n",
    "    \\sum_{i=1}^{N} x_{i}y_{i} - \\sum_{i=1}^{N} x_{i}\\bar{y} + \\sum_{i=1}^{N} x_{i}\\beta_{1}\\bar{x} - \\beta_{1}x^{2}_{i} &=0 \\\\    \n",
    "    \\beta_{1} \\left(x^{2}_{i} - \\sum_{i=1}^{N} x_{i}\\bar{x}\\right) &=\\sum_{i=1}^{N} x_{i}y_{i} - \\sum_{i=1}^{N} x_{i}\\bar{y}\\\\\n",
    "    \\beta_{1} &= \\frac{\\sum_{i=1}^{N} x_{i}y_{i} - \\sum_{i=1}^{N} x_{i}\\bar{y}}{\\left(x^{2}_{i} - \\sum_{i=1}^{N} x_{i}\\bar{x}\\right)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation for $\\beta_{1}$ doesn't look like anything we can recognize, but we can change the SSE we optimized to make this equation look more familiar.\n",
    "The equation we optimized was a function of $\\beta_{0}$ and $\\beta_{1}$, and so adding a constant value that does not include $\\beta_{0}$ or $\\beta_{1}$ would not change the optimal $\\beta$.\n",
    "\n",
    "From each data point, lets subtract $\\bar{x}$ and $\\bar{y}$, called centering our data.\n",
    "Then the above equation becomes\n",
    "\n",
    "\\begin{align}\n",
    "    \\beta_{1} &= \\frac{\\sum_{i=1}^{N} x_{i}y_{i} - \\sum_{i=1}^{N} x_{i}\\bar{y}}{\\left(x^{2}_{i} - \\sum_{i=1}^{N} x_{i}\\bar{x}\\right)}\\\\\n",
    "              &=\\frac{\\sum_{i=1}^{N} (x_{i}-\\bar{x})(y_{i}-\\bar{y}) - \\sum_{i=1}^{N} (x_{i}-\\bar{x})\\bar{y}}{\\sum_{i=1}^{N} (x_{i}-\\bar{x})^{2} - \\bar{x} \\sum_{i=1}^{N} (x_{i}-\\bar{x}) }\\\\\n",
    "              &=\\frac{\\sum_{i=1}^{N} (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\sum_{i=1}^{N} (x_{i}-\\bar{x})^{2} }\\\\\n",
    "              &= \\frac{Cov(X,Y)}{Var(X)}\n",
    "\\end{align}\n",
    "\n",
    "Centering our data, we see the optimal $\\beta_{1}$ is the covariance between $y$ and $x$ divided by the variance of $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also right the above in matrix form.\n",
    "The covariance between X and Y is written \n",
    "\n",
    "$$\n",
    "    Cov(X,Y) = X'y\n",
    "$$\n",
    "where $X = x-\\bar{x}$ and $Y=y-\\bar{y}$, and the variance of X is written \n",
    "\n",
    "$$\n",
    "Var(X) = X'X.\n",
    "$$\n",
    "\n",
    "Then the expression for $\\beta_{1}$ is\n",
    "\n",
    "$$\n",
    "\\beta_{1} = (X'X)^{-1}(X'y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But by adding a column of $1$s to X, we can see that the above expression works for both $\\beta_{1}$ and $\\beta_{0}$.\n",
    "In fact, this expression will work for any design matrix $X$.\n",
    "\n",
    "So we can write\n",
    "\n",
    "$$\n",
    "\\beta = (X'X)^{-1}(X'y)\n",
    "$$\n",
    "\n",
    "To see this more clearly, let's generalize our derivations of $\\beta_{0}$ and $\\beta_{1}$ to multiple $\\beta$s.\n",
    "\n",
    "We first form our SSE for multiple linear regression\n",
    "\n",
    "$$\n",
    "\\text{SSE}(y,X,\\beta_{0},\\beta_{1}) = \\sum_{i=1}^{N} [y_{i} - (\\beta_{0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} + \\cdots \\beta_{n}x_{in} )]^2\\\\\n",
    "$$\n",
    "\n",
    "where $x_{ij}$ is observation $i$ for variable $j$.\n",
    "Taking the derivative for every $\\beta$ and setting equal to $0$ we have\n",
    "\n",
    "For $\\beta_{0}$\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^{N} 1[y_{i} - (\\beta_{0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} + \\cdots \\beta_{n}x_{in})] &= 0\\\\\n",
    "\\sum_{i=1}^{N} (\\beta_{0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} + \\cdots \\beta_{n}x_{in}) &= \\sum_{i=1}^{N} 1 y_{i}\\\\\n",
    "\\end{align}\n",
    "\n",
    "For $\\beta_{1}$\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^{N} x_{i1}[y_{i} - (\\beta_{0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} + \\cdots \\beta_{n}x_{in})] &=0\\\\\n",
    "\\sum_{i=1}^{N} x_{i1} (\\beta_{0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} + \\cdots \\beta_{n}x_{in}) &=\\sum_{i=1}^{N} x_{i1}y_{i}\\\\\n",
    "\\end{align}\n",
    "\n",
    "For $\\beta_{2}$\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^{N} x_{i2}[y_{i} - (\\beta_{0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} + \\cdots \\beta_{n}x_{in})] =0 \\\\\n",
    "\\sum_{i=1}^{N} x_{i2} (\\beta_{0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} + \\cdots \\beta_{n}x_{in}) &=\\sum_{i=1}^{N} x_{i2}y_{i}\\\\\n",
    "\\end{align}\n",
    "\n",
    "For $\\beta_{n}$\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^{N} x_{in}[y_{i} - (\\beta_{0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} + \\cdots \\beta_{n}x_{in})] =0 \\\\\n",
    "\\sum_{i=1}^{N} x_{in} (\\beta_{0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} + \\cdots \\beta_{n}x_{in}) &=\\sum_{i=1}^{N} x_{in}y_{i}\\\\\n",
    "\\end{align}\n",
    "\n",
    "The right hand side of this system of equations can be rewritten as\n",
    "\n",
    "$$\n",
    "X'y,\n",
    "$$\n",
    "\n",
    "and the left hand side can be rewritten as \n",
    "\n",
    "$$\n",
    "    (X'X)\\beta.\n",
    "$$\n",
    "\n",
    "Our system of equations is then\n",
    "\n",
    "$$\n",
    " (X'X)\\beta = X'y.\n",
    "$$\n",
    "\n",
    "We can solve for $\\beta$ by left multiplying each side by $(X'X)^{-1}$\n",
    "\n",
    "$$\n",
    "\\beta = (X'X)^{-1}X'y\n",
    "$$\n",
    "\n",
    "and arriving at the same solution for multiple linear regression that we found with simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the above equation $(X'X)^{-1}X'y$ recovers the optimal $\\beta$s using R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 6 Ã— 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>x</th><th scope=col>y</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td> 0.9958723</td><td> 8.2420054</td></tr>\n",
       "\t<tr><td>-0.6556163</td><td> 2.3114202</td></tr>\n",
       "\t<tr><td>-0.9176787</td><td> 4.0842076</td></tr>\n",
       "\t<tr><td> 0.1963727</td><td>-5.5386897</td></tr>\n",
       "\t<tr><td> 1.0309346</td><td> 2.5166174</td></tr>\n",
       "\t<tr><td> 1.2610719</td><td>-0.5388713</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 Ã— 2\n",
       "\\begin{tabular}{r|ll}\n",
       " x & y\\\\\n",
       " <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t  0.9958723 &  8.2420054\\\\\n",
       "\t -0.6556163 &  2.3114202\\\\\n",
       "\t -0.9176787 &  4.0842076\\\\\n",
       "\t  0.1963727 & -5.5386897\\\\\n",
       "\t  1.0309346 &  2.5166174\\\\\n",
       "\t  1.2610719 & -0.5388713\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 Ã— 2\n",
       "\n",
       "| x &lt;dbl&gt; | y &lt;dbl&gt; |\n",
       "|---|---|\n",
       "|  0.9958723 |  8.2420054 |\n",
       "| -0.6556163 |  2.3114202 |\n",
       "| -0.9176787 |  4.0842076 |\n",
       "|  0.1963727 | -5.5386897 |\n",
       "|  1.0309346 |  2.5166174 |\n",
       "|  1.2610719 | -0.5388713 |\n",
       "\n"
      ],
      "text/plain": [
       "  x          y         \n",
       "1  0.9958723  8.2420054\n",
       "2 -0.6556163  2.3114202\n",
       "3 -0.9176787  4.0842076\n",
       "4  0.1963727 -5.5386897\n",
       "5  1.0309346  2.5166174\n",
       "6  1.2610719 -0.5388713"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"CUBIC REGRESSION\"\n",
      "\n",
      "Call:\n",
      "lm(formula = y ~ x + I(x^2) + I(x^3), data = data)\n",
      "\n",
      "Coefficients:\n",
      "(Intercept)            x       I(x^2)       I(x^3)  \n",
      "     0.2994       2.2877       1.0962      -1.4120  \n",
      "\n",
      "[1] \"DESIGN MATRIX\"\n",
      "     ones          x         x2           x3\n",
      "[1,]    1  0.9958723 0.99176166  0.987667975\n",
      "[2,]    1 -0.6556163 0.42983269 -0.281805304\n",
      "[3,]    1 -0.9176787 0.84213426 -0.772808705\n",
      "[4,]    1  0.1963727 0.03856225  0.007572574\n",
      "[5,]    1  1.0309346 1.06282620  1.095704329\n",
      "[6,]    1  1.2610719 1.59030227  2.005485460\n",
      "[1] \"OPTIMAL BETAS\"\n",
      "        [,1]\n",
      "ones  0.2994\n",
      "x     2.2877\n",
      "x2    1.0962\n",
      "x3   -1.4120\n"
     ]
    }
   ],
   "source": [
    "data <- read.csv('polynomialData.csv')\n",
    "head(data)\n",
    "\n",
    "cubicRegression <- lm(y~x+I(x^2)+I(x^3),data=data)\n",
    "\n",
    "print(\"CUBIC REGRESSION\")\n",
    "print(cubicRegression)\n",
    "\n",
    "y <- data$y\n",
    "ones = rep(1,length(data$x))\n",
    "x    = data$x\n",
    "x2   = data$x^2\n",
    "x3   = data$x^3\n",
    "\n",
    "print(\"DESIGN MATRIX\")\n",
    "X = cbind(ones,x,x2,x3)\n",
    "print(head(X))\n",
    "\n",
    "print(\"OPTIMAL BETAS\")\n",
    "optimalBetas <- solve(t(X)%*%X,t(X)%*%y)  # this is the same as solving our system of equatons.\n",
    "print(round(optimalBetas,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beta coefficients we found from running a cubic regression match the beta coefficients from solving the system of equations above that minimize the SSE. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
